{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kaggle Titanic Machine Learning\n- source of competition: https://www.kaggle.com/c/titanic\n- Data Dictionary: https://www.kaggle.com/c/titanic/data\n- useful link for saving to GitHub: https://www.kaggle.com/questions-and-answers/72234"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing libraries\n%matplotlib inline\nimport numpy as np \nimport pandas as pd \nimport pandas_profiling\n\n# Setting Random Seed For Reproducibility\nimport random\nrandom.seed(123)\n\n# Displaying Max rows\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 100)\n\n# Listing Files\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/titanic/train.csv')\ndf_test = pd.read_csv('/kaggle/input/titanic/test.csv') # for final evaluation/submission only","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Wrangling/Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating train/val/test split prior to transformations (avoid data leakage)\n\nX = df_train.drop(['Survived'],axis = 1)\ny = df_train.Survived\n\nfrom sklearn.model_selection import train_test_split\n\n# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.15, random_state = 3) # test set 15% train\n# X_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size = 0.15, random_state = 3) #validation set 15% train\n\n# Model already selection, rerunning with 99% of data \nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.01, random_state = 3) # test set 1% train\nX_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size = 0.01, random_state = 3) #validation set 1% train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [X_train,X_test,X_val]:\n    print(i.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nGeneral thoughts based on the profile below\n- PassengerId - removing due to ID variable\n- Missing values: Age, Cabin, Fare, Embarked\n- Correlations in Fare-Class-Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"#combining the train feature/target data for EDA/Data Wrangling\n\ndf_train_split = pd.concat([X_train, y_train], axis = 1)\ndf_train_split.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making use of the profile package for EDA plots/stats/...\n\nprofile = pandas_profiling.ProfileReport(df_train_split, title = \"EDA Profile Train Data Report\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile.to_widgets()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Wrangling and Feature Engineering\n- only on training dataset, will use a pipeline for val/test and final submission test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing Values Handling\nprint(df_train_split.Embarked.value_counts())\n\n#Embarked only 1 missing, fill with most common of S, C, Q (will be S)\ndf_train_split.Embarked = df_train_split.Embarked.fillna(df_train_split.Embarked.value_counts().index[0]) #using value_counts top record","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping passenger id (is an id)\ndf_train_split.drop(['PassengerId'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(categories='auto')\nfeature_array = ohe.fit_transform(df_train_split[['Parch','Pclass','Sex','SibSp','Embarked']]).toarray()\n#feature_labels = ohe.categories_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.DataFrame(feature_array, columns=ohe.get_feature_names())\nprint(features.shape)\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_split = df_train_split.drop(['Parch','Pclass','Sex','SibSp','Embarked'], axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_split = pd.concat([df_train_split.reset_index(drop=True),features.reset_index(drop=True)], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding the Age Missing Values that are 'S' with the training data median Age \nmedian_age_train = df_train_split.Age.median()\ndf_train_split['Age'] = df_train_split['Age'].apply(lambda x : median_age_train if pd.isnull(x) else x)\ndf_train_split.Age.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cabin Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#INPROGRESS #Missing Values Cabin - taking the initial value\ndf_train_split.Cabin = df_train_split[['Cabin']].fillna(value= 'Z')\ndf_train_split['Cabin_augment'] = df_train_split.Cabin.apply(lambda x : x[0]) # augmenting dataset, only want the first letter (numbers not matter)\ndf_train_split.Cabin_augment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_split[['Cabin_augment','Fare']].groupby(['Cabin_augment']).mean().round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nohe_Cabin_augment = OneHotEncoder(categories='auto')\nfeature_array_Cabin_augment = ohe_Cabin_augment.fit_transform(df_train_split[['Cabin_augment']]).toarray()\n#feature_labels = ohe.categories_\nfeatures_Cabin_augment = pd.DataFrame(feature_array_Cabin_augment, columns=ohe_Cabin_augment.get_feature_names())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_split.drop(['Cabin','Cabin_augment'], axis =1, inplace = True)\ndf_train_split = pd.concat([df_train_split,features_Cabin_augment], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_split.drop(['Name','Ticket'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_split.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking that all missing values are taken care of\nprint(df_train_split.isna().sum().sum())\ndf_train_split.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_split_X = df_train_split.drop(['Survived'],axis = 1)\ndf_train_split_y = df_train_split[['Survived']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Imbalance Correction via SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\nimport collections\n\nprint('Prior', collections.Counter(np.squeeze(df_train_split_y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Smote Operation\n\nsmote_instance = ADASYN(random_state=0)\nX_train_resampled, y_train_resampled = smote_instance.fit_sample(df_train_split_X, df_train_split_y)\n\nprint('Post', collections.Counter(np.squeeze(y_train_resampled)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_resampled = sc.fit_transform(X_train_resampled)\nX_train_resampled.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performing Save Data Wrangling Steps on the Val/Test Data\n\n#combining the train feature/target data for EDA/Data Wrangling\ndf_val_split = pd.concat([X_val, y_val], axis = 1)\ndf_val_split.Embarked = df_val_split.Embarked.fillna('S')\ndf_val_split.drop(['PassengerId'], axis = 1, inplace = True)\n\nfeature_array = ohe.transform(df_val_split[['Parch','Pclass','Sex','SibSp','Embarked']]).toarray()\nfeatures = pd.DataFrame(feature_array, columns=ohe.get_feature_names())\ndf_val_split = df_val_split.drop(['Parch','Pclass','Sex','SibSp','Embarked'], axis = 1)\ndf_val_split = pd.concat([df_val_split.reset_index(drop=True),features.reset_index(drop=True)], axis = 1)\n\ndf_val_split['Age'] = df_val_split['Age'].apply(lambda x : median_age_train if pd.isnull(x) else x)\n\ndf_val_split.Cabin = df_val_split[['Cabin']].fillna(value= 'Z')\ndf_val_split['Cabin_augment'] = df_val_split.Cabin.apply(lambda x : x[0])\n\nfeature_array_Cabin_augment = ohe_Cabin_augment.transform(df_val_split[['Cabin_augment']]).toarray()\nfeatures_Cabin_augment = pd.DataFrame(feature_array_Cabin_augment, columns=ohe_Cabin_augment.get_feature_names())\n\ndf_val_split.drop(['Cabin','Cabin_augment'], axis =1, inplace = True)\ndf_val_split = pd.concat([df_val_split,features_Cabin_augment], axis = 1)\n\ndf_val_split.drop(['Name','Ticket'], axis = 1, inplace = True)\n\nprint(df_val_split.shape)\ndf_val_split.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_val_split_X = df_val_split.drop(['Survived'],axis = 1)\ndf_val_split_y = df_val_split[['Survived']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_val_split_X = sc.transform (df_val_split_X)\ndf_val_split_X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Developement "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Baseline Model \nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\n\neval_set = [(df_val_split_X,df_val_split_y.values.ravel())]\n\nmodel_xgb = xgb.XGBClassifier(learning_rate = 0.01)\nmodel_xgb.fit(X_train_resampled, y_train_resampled.values.ravel(), early_stopping_rounds=10, eval_metric=\"error\", eval_set= eval_set,verbose = 0)\n\nprint(\"Training Accuracy:\", accuracy_score(model_xgb.predict(X_train_resampled),y_train_resampled))\nprint(\"Validation Accuracy:\", accuracy_score(model_xgb.predict(df_val_split_X),df_val_split_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding Parameter Tuning\nfrom sklearn.model_selection import GridSearchCV\n\neval_set = [(df_val_split_X,df_val_split_y.values.ravel())]\n\nparam_grid = {\n    \"learning_rate\": [0.1,0.05],\n    'max_depth': [2,3,4,5,6],\n    'min_child_weight': [1, 2,4,6,8,10],\n    'subsample': [0.5, 0.7, 0.9],\n    'n_estimators': [5, 30, 100, 250, 500],\n}\n\ngrid_clf = GridSearchCV(xgb.XGBClassifier() , param_grid, scoring='accuracy', cv=None)\ngrid_clf.fit(X_train_resampled, y_train_resampled.values.ravel() , early_stopping_rounds=10 , eval_metric=\"error\", eval_set= eval_set,verbose = False)\n\nbest_parameters = grid_clf.best_params_\n\nprint('Grid Search found the following optimal parameters: ')\nfor param_name in sorted(best_parameters.keys()):\n    print('%s: %r' % (param_name, best_parameters[param_name]))\n    \nprint(\"Training Accuracy:\", accuracy_score(grid_clf.predict(X_train_resampled),y_train_resampled.values.ravel()))\nprint(\"Validation Accuracy:\", accuracy_score(grid_clf.predict(df_val_split_X),df_val_split_y.values.ravel()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\n\ngnb.fit(X_train_resampled, y_train_resampled.values.ravel())\n\nprint(\"Training Accuracy:\", accuracy_score(gnb.predict(X_train_resampled),y_train_resampled))\nprint(\"Validation Accuracy:\", accuracy_score(gnb.predict(df_val_split_X),df_val_split_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclf_log = LogisticRegression(random_state=0, max_iter = 1000).fit(X_train_resampled, y_train_resampled.values.ravel())\n\nprint(\"Training Accuracy:\", accuracy_score(clf_log.predict(X_train_resampled),y_train_resampled))\nprint(\"Validation Accuracy:\", accuracy_score(clf_log.predict(df_val_split_X),df_val_split_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Stacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"from warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\n\nestimators = [\n    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n    ('knn', KNeighborsClassifier()),\n    ('cart',DecisionTreeClassifier()),\n    ('svr', make_pipeline(LinearSVC(random_state=42))),\n    ('svc', SVC(gamma='auto'))]\n\nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n\nclf_stack = clf.fit(X_train_resampled, y_train_resampled.values.ravel())\n\nprint(\"Training Accuracy:\", accuracy_score(clf_stack.predict(X_train_resampled),y_train_resampled.values.ravel()))\nprint(\"Validation Accuracy:\", accuracy_score(clf_stack.predict(df_val_split_X),df_val_split_y.values.ravel()))\n\nprint(\"Confusion Matrix:\\n\",confusion_matrix(clf_stack.predict(df_val_split_X),df_val_split_y.values.ravel()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gridsearch (next steps)\n\n# estimators = [\n#     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n#     ('knn', KNeighborsClassifier()),\n#     ('cart',DecisionTreeClassifier()),\n#     ('svr', make_pipeline(LinearSVC(random_state=42)))]\n\n# sclf = StackingClassifier(estimators= estimators , final_estimator= LogisticRegression()) # =DecisionTreeClassifier())\n\n# # params = {'rf__n_estimators': [5,10,20],\n# #           'rf__max_features': [5,10,20],\n# #           'rf__max_depth': [1,3,5,7],\n# #           'rf__min_samples_leaf': [10,25,50],\n# #           'knn__n_neighbors': [3,5,7],\n# #           'knn__algorithm':['ball_tree','kd_tree']}\n\n# params = {'rf__n_estimators': [5,10,20],\n#           'rf__max_features': [5,10],\n#           'rf__max_depth': [3,5,7],\n#           'knn__n_neighbors': [3,5],\n#           'knn__algorithm':['ball_tree','kd_tree']}\n\n# grid = GridSearchCV(estimator=sclf, param_grid=params, cv=5)\n# grid.fit(X_train_resampled, y_train_resampled.values.ravel())\n\n\n# print(\"Training Accuracy:\", accuracy_score(grid.predict(X_train_resampled),y_train_resampled.values.ravel()))\n# print(\"Validation Accuracy:\", accuracy_score(grid.predict(df_val_split_X),df_val_split_y.values.ravel()))\n\n# print(\"Confusion Matrix:\\n\",confusion_matrix(grid.predict(df_val_split_X),df_val_split_y.values.ravel()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting the test submission data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#transforming the test data like train (make a pipeline later...)\n\ndf_test = pd.read_csv('/kaggle/input/titanic/test.csv') # for final evaluation/submission only\ndf_test_ids = df_test[['PassengerId']] # for creating the csv\n\ndf_test.Embarked = df_test.Embarked.fillna('S')\ndf_test.drop(['PassengerId'], axis = 1, inplace = True)\n\ndf_test['Fare'].fillna((df_test['Fare'].mean()), inplace=True) # fare in test has 1 missing value, set to mean\ndf_test.loc[df_test['Parch'] == 9, 'Parch'] = 0 #9 is contained in Parch only in the test, so setting to most common, cause next ohe transfor to break\n\nfeature_array = ohe.transform(df_test[['Parch','Pclass','Sex','SibSp','Embarked']]).toarray()\nfeatures = pd.DataFrame(feature_array, columns=ohe.get_feature_names())\ndf_test = df_test.drop(['Parch','Pclass','Sex','SibSp','Embarked'], axis = 1)\ndf_test = pd.concat([df_test.reset_index(drop=True),features.reset_index(drop=True)], axis = 1)\n\ndf_test['Age'] = df_test['Age'].apply(lambda x : median_age_train if pd.isnull(x) else x)\n\ndf_test.Cabin = df_test[['Cabin']].fillna(value= 'Z')\ndf_test['Cabin_augment'] = df_test.Cabin.apply(lambda x : x[0])\n\nfeature_array_Cabin_augment = ohe_Cabin_augment.transform(df_test[['Cabin_augment']]).toarray()\nfeatures_Cabin_augment = pd.DataFrame(feature_array_Cabin_augment, columns=ohe_Cabin_augment.get_feature_names())\n\ndf_test.drop(['Cabin','Cabin_augment'], axis =1, inplace = True)\ndf_test = pd.concat([df_test,features_Cabin_augment], axis = 1)\n\ndf_test.drop(['Name','Ticket'], axis = 1, inplace = True)\n\ndf_test = sc.transform(df_test)\n\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting using the clf_stack\n\npredictions = clf_stack.predict(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dynamic Filename Creation\n\nfrom datetime import datetime\nfrom pytz import timezone\n\ntimestr = datetime.now(timezone('EST')).strftime(\"%Y%m%d_%H%M%S\")\nfile_name = 'rad_submission_' + timestr + '.csv'\nfile_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating the sumission object CSV\n\ndf_submit = pd.DataFrame(data=np.column_stack((df_test_ids, predictions)),columns=['PassengerId','Survived'])\ndf_submit.to_csv(file_name, index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Potential Next Steps / Changes to Consider / Resources Referenced\n- Potentially use K-Fold Cross validation due to small size \n- Feature engineering (Class x sex), (Class x Parch)\n- Add more model types, more hyperparameters\n- Add model stacking\n- https://alexforrest.github.io/you-might-be-leaking-data-even-if-you-cross-validate.html\n- https://machinelearningmastery.com/data-preparation-without-data-leakage/\n- http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}